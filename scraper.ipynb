{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query source and output result path\n",
    "student_id = \"M11207321\"\n",
    "query_path = f\"./queries/{student_id}_queries.txt\"\n",
    "results_path = \"./results\"\n",
    "\n",
    "# Web scraping target URL\n",
    "search_url = \"https://www.tw.coupang.com/search?q=\"\n",
    "\n",
    "# Scraping parameter settings\n",
    "short_time_sleep = 1\n",
    "medium_time_sleep = 3\n",
    "long_time_sleep = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries from file\n",
    "def read_queries(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "# Check if the webpage is accessible\n",
    "def check_access(driver):\n",
    "    try:\n",
    "        text = driver.find_element(By.XPATH, '/html/body/h1')\n",
    "        if text.text == 'Access Denied':\n",
    "            return False\n",
    "        else: \n",
    "            return True\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "# Scroll the webpage to the bottom\n",
    "def scroll_to_bottom(driver, pause_time=3):\n",
    "    # Get the current scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Pause to allow any new content to load\n",
    "    time.sleep(pause_time)\n",
    "    \n",
    "    # Get the new scroll height after scrolling\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # If the new height is greater than the last height, new content has loaded\n",
    "    if not (new_height > last_height):\n",
    "        driver.execute_script(\"window.scrollBy(0, -100);\")\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "# Get all items from pages   \n",
    "def get_all_items(driver):\n",
    "    try:\n",
    "        items = driver.find_elements(By.XPATH, '/html/body/div[1]/div[2]/main/div/div/div[3]/div/div/a')\n",
    "        return items\n",
    "    except:\n",
    "        print(\"No items found.\")\n",
    "\n",
    "# Extract all item informations to a dataframe   \n",
    "def extract_item_info(items):\n",
    "    print(\"Extracting item information...\")\n",
    "    data = []\n",
    "    for i, item in enumerate(items):\n",
    "        try:\n",
    "            item_name = item.find_element(By.XPATH, 'div[2]').text\n",
    "            item_url = item.get_attribute('href')\n",
    "            # price is in one of two possible XPaths\n",
    "            try:\n",
    "                item_price = item.find_element(By.XPATH, 'div[3]/div[2]/span/span').text # on sale.\n",
    "            except:\n",
    "                try:\n",
    "                    item_price = item.find_element(By.XPATH, 'div[3]/div[1]/span/span').text # not on sale.\n",
    "                except:\n",
    "                    item_price = \"null\"\n",
    "            \n",
    "            data.append({\n",
    "                'product_name': item_name,\n",
    "                'product_price': item_price,\n",
    "                'product_url': item_url\n",
    "            })\n",
    "        except:\n",
    "            print(f\"Error extracting item {i}.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Chrome options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the webpage\n",
    "driver = webdriver.Chrome()\n",
    "time.sleep(short_time_sleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start web scraping.\n",
    "#### (During scraping, you may open other windows, but do not close or minimize the Chrome window that is performing the scraping.)\n",
    "#### (Make sure the screen remains on while the scraper is running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start scraping 筆電...\n",
      "Extracting item information...\n",
      "Results for 筆電 have been saved to ./results\\M11207321_筆電.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 衣服...\n",
      "Extracting item information...\n",
      "Results for 衣服 have been saved to ./results\\M11207321_衣服.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 餅乾...\n",
      "Extracting item information...\n",
      "Results for 餅乾 have been saved to ./results\\M11207321_餅乾.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 洗衣精...\n",
      "Extracting item information...\n",
      "Results for 洗衣精 have been saved to ./results\\M11207321_洗衣精.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 衛生紙...\n",
      "Extracting item information...\n",
      "Results for 衛生紙 have been saved to ./results\\M11207321_衛生紙.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 衣架...\n",
      "Extracting item information...\n",
      "Results for 衣架 have been saved to ./results\\M11207321_衣架.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 手機殼...\n",
      "Extracting item information...\n",
      "Results for 手機殼 have been saved to ./results\\M11207321_手機殼.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 維他命...\n",
      "Extracting item information...\n",
      "Results for 維他命 have been saved to ./results\\M11207321_維他命.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 口罩...\n",
      "Extracting item information...\n",
      "Results for 口罩 have been saved to ./results\\M11207321_口罩.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Start scraping 吹風機...\n",
      "Extracting item information...\n",
      "Results for 吹風機 have been saved to ./results\\M11207321_吹風機.csv\n",
      "Sleeping for a while...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Main scraping process\n",
    "queries = read_queries(query_path)\n",
    "for query in queries:\n",
    "    # Get all csv files in the results folder, if exists pass the query\n",
    "    csv_files = [f for f in os.listdir(results_path) if f.endswith('.csv')]\n",
    "    search_string = query\n",
    "    all_contain_string = any(search_string in file_name for file_name in csv_files)\n",
    "    if all_contain_string:\n",
    "        print(f\"Results for {query} have already been scraped. Skipping...\\n\")\n",
    "        continue\n",
    "\n",
    "    # Search for the query\n",
    "    driver.get(search_url + query)\n",
    "    time.sleep(medium_time_sleep)\n",
    "    status = check_access(driver)\n",
    "    if status:\n",
    "        print(f\"Start scraping {query}...\")\n",
    "    else:\n",
    "        print(f\"Some error occurred while scraping {query}.\")\n",
    "        continue\n",
    "    while status:\n",
    "        status = check_access(driver)\n",
    "        if status:  # Only proceed to scroll if check_access is True\n",
    "            status = scroll_to_bottom(driver, medium_time_sleep + random.random()) # add time noise\n",
    "\n",
    "    # Process the items\n",
    "    items = get_all_items(driver)\n",
    "    items_df = extract_item_info(items)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    file_path = os.path.join(results_path, f\"{student_id}_{query}.csv\")\n",
    "    items_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Results for {query} have been saved to {file_path}\")\n",
    "    time.sleep(long_time_sleep + random.random() * 10)\n",
    "    print(\"Sleeping for a while...\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
